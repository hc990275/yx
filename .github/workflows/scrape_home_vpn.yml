name: 抓取家宽 VPN (持久化+倒序+严选)

on:
  workflow_dispatch:
  schedule:
    - cron: '*/5 * * * *'

jobs:
  scrape-persist-reverse:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      
    steps:
      - name: 检出代码
        uses: actions/checkout@v4

      - name: 安装工具
        run: |
          sudo apt-get update
          sudo apt-get install -y lynx

      - name: 抓取、过滤并合并数据
        env:
          VPN_SOURCE_URL: ${{ secrets.VPN_SOURCE_URL }}
        run: |
          mkdir -p 家宽
          cd 家宽
          
          # =======================================================
          # 第一步：获取最新数据 (New Data)
          # =======================================================
          echo "1. 正在抓取新数据..."
          # 使用 lynx 转换为文本，保留整行信息
          lynx -dump -width=300 "$VPN_SOURCE_URL" > raw_latest.txt

          # 提取包含有效 IP 的行，并进行严格过滤
          # 1. grep: 提取包含数字开头的行 (假设IP在行首)
          # 2. awk: 严格排除 $1 (第一列IP) 以 "219." 开头的行
          #    同时给每一行加上表格竖线 "| ... |"
          grep -E "^[0-9]{1,3}\.[0-9]{1,3}\." raw_latest.txt | \
          awk '$1 !~ /^219\./ {print "| " $0 " |"}' > new_entries.txt
          
          echo "本次抓取到的非219数据量: $(wc -l < new_entries.txt)"

          # =======================================================
          # 第二步：读取旧数据 (Old Data)
          # =======================================================
          echo "2. 读取历史数据..."
          touch 非219IP.md # 确保文件存在
          
          # 从现有的 md 文件中提取数据行 (排除表头和空行)
          # grep "^| [0-9]" 表示只取以 "| 数字" 开头的表格行
          grep "^| [0-9]" 非219IP.md > history_entries.txt || true

          # =======================================================
          # 第三步：合并、去重、倒序 (Merge & Sort)
          # =======================================================
          echo "3. 合并新旧数据并倒序排列..."
          
          # 逻辑：将【新数据】放在【旧数据】前面，然后去重
          # 这样最新抓到的会排在最上面 (时间倒序效果)
          cat new_entries.txt history_entries.txt > combined.txt
          
          # awk 去重：!seen[$0]++ 确保完全相同的行只保留第一次出现的
          # 因为新数据在上面，所以重复时保留的是新数据的位置（还是在上面）
          awk '!seen[$0]++' combined.txt > final_list.txt

          # 如果你指的是 "IP数值倒序" (9.x.x.x 排在 1.x.x.x 前面)，
          # 请取消下面这行注释 (目前默认是按"发现时间"倒序，新的在最前)
          # sort -r -V final_list.txt -o final_list.txt

          # =======================================================
          # 第四步：写入文件
          # =======================================================
          echo "4. 生成最终文件..."
          
          # 写入表头
          echo "# 非 219 IP 永久记录库" > 非219IP.md
          echo "" >> 非219IP.md
          echo "> 最后更新: $(date '+%Y-%m-%d %H:%M:%S') | 累计数量: $(wc -l < final_list.txt)" >> 非219IP.md
          echo "" >> 非219IP.md
          echo "| IP / 端口 / 地区信息 |" >> 非219IP.md
          echo "| :--- |" >> 非219IP.md
          
          # 写入处理好的数据列表
          cat final_list.txt >> 非219IP.md
          
          # 清理临时文件
          rm raw_latest.txt new_entries.txt history_entries.txt combined.txt final_list.txt

      - name: 提交并推送
        run: |
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action@github.com"
          
          if [[ -n $(git status -s) ]]; then
            git add .
            git commit -m "更新: 非219 IP库 (持久化累加) [$(date '+%H:%M')]"
            git push
          else
            echo "没有新数据加入，跳过提交。"
          fi
