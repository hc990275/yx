name: 抓取家宽 VPN (双列表更新版)

on:
  workflow_dispatch:
  schedule:
    - cron: '*/5 * * * *'  # 每5分钟运行一次

jobs:
  scrape-dual-update:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      
    steps:
      - name: 检出代码
        uses: actions/checkout@v4

      - name: 设置 Python 环境
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: 安装依赖
        run: pip install requests beautifulsoup4

      - name: 生成并运行双轨抓取脚本
        env:
          VPN_SOURCE_URL: ${{ secrets.VPN_SOURCE_URL }}
        run: |
          cat <<'EOF' > run_scraper.py
          import os
          import re
          import requests
          from bs4 import BeautifulSoup
          import datetime

          # 默认配置
          URL = os.environ.get("VPN_SOURCE_URL", "https://ipspeed.info/free-l2tpipsec.php")
          FILE_NON_219 = "家宽/非219IP.md"
          FILE_README = "家宽/README.md"

          def get_data_from_web():
              """
              从网页抓取所有有效数据，返回两个列表：
              1. all_lines: 包含所有抓取到的数据（用于 README）
              2. new_non_219_lines: 仅包含非 219 的新数据（用于持久化）
              """
              headers = {
                  "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
              }
              all_lines = []
              new_non_219_lines = []
              
              try:
                  print(f"正在抓取: {URL}")
                  resp = requests.get(URL, headers=headers, timeout=30)
                  resp.encoding = 'utf-8'
                  soup = BeautifulSoup(resp.text, 'html.parser')
                  
                  table = soup.find('table')
                  if not table: return [], []

                  rows = table.find_all('tr')
                  for row in rows:
                      cols = row.find_all('td')
                      if len(cols) >= 2:
                          location = cols[0].get_text(strip=True)
                          ip = cols[1].get_text(strip=True)
                          
                          # 基础 IP 校验
                          if re.match(r'^\d{1,3}(\.\d{1,3}){3}$', ip):
                              line_str = f"| {ip} | {location} |"
                              
                              # 1. 存入全量列表
                              all_lines.append(line_str)
                              
                              # 2. 筛选非 219 存入精选列表
                              if not ip.startswith("219."):
                                  new_non_219_lines.append(line_str)
                                  
                  return all_lines, new_non_219_lines
              except Exception as e:
                  print(f"抓取异常: {e}")
                  return [], []

          def update_readme(all_data):
              """更新 README.md (覆盖更新，显示最新抓取结果)"""
              if not all_data: return
              
              timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
              with open(FILE_README, "w", encoding="utf-8") as f:
                  f.write(f"# 家宽 VPN 完整列表 (最新快照)\n\n")
                  f.write(f"> 更新时间: {timestamp} (UTC) | 当前在线: {len(all_data)}\n\n")
                  f.write(f"| IP 地址 | 地区信息 |\n")
                  f.write(f"| :--- | :--- |\n")
                  for line in all_data:
                      f.write(f"{line}\n")
              print(f"✅ README.md 已更新 (包含 {len(all_data)} 条数据)")

          def update_non_219(new_data):
              """更新 非219IP.md (持久化 + 智能去重)"""
              # 读取旧数据
              old_lines = []
              if os.path.exists(FILE_NON_219):
                  with open(FILE_NON_219, "r", encoding="utf-8") as f:
                      for line in f:
                          line = line.strip()
                          # 只要包含 IP 的表格行都读进来
                          if line.startswith("|") and re.search(r'\d+\.\d+\.\d+\.\d+', line):
                              old_lines.append(line)

              # 合并：新数据在前
              candidates = new_data + old_lines
              
              unique_lines = []
              seen_ips = set()
              ip_extract_pattern = re.compile(r'\|\s*(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s*\|')

              for line in candidates:
                  match = ip_extract_pattern.search(line)
                  if match:
                      ip = match.group(1)
                      if ip not in seen_ips:
                          seen_ips.add(ip)
                          unique_lines.append(line)
              
              timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
              with open(FILE_NON_219, "w", encoding="utf-8") as f:
                  f.write(f"# 非 219 IP 永久记录库\n\n")
                  f.write(f"> 更新时间: {timestamp} (UTC) | 有效记录: {len(unique_lines)}\n\n")
                  f.write(f"| IP 地址 | 地区信息 |\n")
                  f.write(f"| :--- | :--- |\n")
                  for line in unique_lines:
                      f.write(f"{line}\n")
              print(f"✅ 非219IP.md 已更新 (总记录 {len(unique_lines)} 条)")

          def main():
              os.makedirs("家宽", exist_ok=True)

              # 1. 抓取
              all_lines, new_non_219_lines = get_data_from_web()
              
              if not all_lines:
                  print("⚠️ 本次未抓取到任何数据")
                  return

              # 2. 更新 完整列表 (README.md)
              update_readme(all_lines)

              # 3. 更新 精选列表 (非219IP.md)
              update_non_219(new_non_219_lines)

          if __name__ == "__main__":
              main()
          EOF

          python run_scraper.py

      - name: 提交更改
        run: |
          git config --global user.name "GitHub Action Bot"
          git config --global user.email "action@github.com"
          
          # 检查是否有任何文件变动
          if [[ -n $(git status -s) ]]; then
            git add .
            git commit -m "自动更新: 双列表同步 (完整列表+非219库) [$(date '+%H:%M')]"
            git push
          else
            echo "无变动"
          fi
